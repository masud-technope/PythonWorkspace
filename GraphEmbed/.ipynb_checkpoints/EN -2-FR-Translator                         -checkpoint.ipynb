{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/e4/0c186661604e70f4be57d44510115e63818189900cb0262bc5cd541bab3e/sacrebleu-1.3.5.tar.gz\n",
      "Collecting typing (from sacrebleu)\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/bd/eee1157fc2d8514970b345d69cb9975dcd1e42cd7e61146ed841f6e68309/typing-3.6.6-py3-none-any.whl\n",
      "Building wheels for collected packages: sacrebleu\n",
      "  Building wheel for sacrebleu (setup.py): started\n",
      "  Building wheel for sacrebleu (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\MasudRahman\\AppData\\Local\\pip\\Cache\\wheels\\f5\\21\\da\\3e3dea18eb53af3b56d99b4b6ae50c91c89c8a7f422d676ee1\n",
      "Successfully built sacrebleu\n",
      "Installing collected packages: typing, sacrebleu\n",
      "Successfully installed sacrebleu-1.3.5 typing-3.6.6\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Could not import signal.SIGPIPE (this is expected on Windows machines)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import sacrebleu\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tf.enable_eager_execution must be called at program startup.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ddf3115bdcc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[1;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[0;32m   5459\u001b[0m         \u001b[0mdevice_policy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice_policy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5460\u001b[0m         \u001b[0mexecution_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5461\u001b[1;33m         server_def=None)\n\u001b[0m\u001b[0;32m   5462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36menable_eager_execution_internal\u001b[1;34m(config, device_policy, execution_mode, server_def)\u001b[0m\n\u001b[0;32m   5514\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgraph_mode_has_been_used\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5515\u001b[0m       raise ValueError(\n\u001b[1;32m-> 5516\u001b[1;33m           \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[0;32m   5517\u001b[0m   \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_execution_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEAGER_MODE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5518\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
     ]
    }
   ],
   "source": [
    "# tf.enable_eager_execution() #should be called at startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  (\"Do you want a cup of coffee?\", \"¿Quieres una taza de café?\"),\n",
    "  (\"I've had coffee already.\", \"Ya tomé café.\"),\n",
    "  (\"Can I get you a coffee?\", \"¿Quieres que te traiga un café?\"),\n",
    "  (\"Please give me some coffee.\", \"Dame algo de café por favor.\"),\n",
    "  (\"Would you like me to make coffee?\", \"¿Quieres que prepare café?\"),\n",
    "  (\"Two coffees, please.\", \"Dos cafés, por favor.\"),\n",
    "  (\"How about a cup of coffee?\", \"¿Qué tal una taza de café?\"),\n",
    "  (\"I drank two cups of coffee.\", \"Me tomé dos tazas de café.\"),\n",
    "  (\"Would you like to have a cup of coffee?\", \"¿Te gustaría tomar una taza de café?\"),\n",
    "  (\"There'll be coffee and cake at five.\", \"A las cinco habrá café y un pastel.\"),\n",
    "  (\"Another coffee, please.\", \"Otro café, por favor.\"),\n",
    "  (\"I made coffee.\", \"Hice café.\"),\n",
    "  (\"I would like to have a cup of coffee.\", \"Quiero beber una taza de café.\"),\n",
    "  (\"Do you want me to make coffee?\", \"¿Quieres que haga café?\"),\n",
    "  (\"It is hard to wake up without a strong cup of coffee.\", \"Es difícil despertarse sin una taza de café fuerte.\"),\n",
    "  (\"All I drank was coffee.\", \"Todo lo que bebí fue café.\"),\n",
    "  (\"I've drunk way too much coffee today.\", \"He bebido demasiado café hoy.\"),\n",
    "  (\"Which do you prefer, tea or coffee?\", \"¿Qué prefieres, té o café?\"),\n",
    "  (\"There are many kinds of coffee.\", \"Hay muchas variedades de café.\"),\n",
    "  (\"I will make some coffee.\",\t\"Prepararé algo de café.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "  # for details, see https://www.tensorflow.org/alpha/tutorials/sequences/nmt_with_attention\n",
    "  s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "  s = re.sub(r'[\" \"]+', \" \", s)\n",
    "  s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
    "  s = s.strip()\n",
    "  s = '<start> ' + s + ' <end>'\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ('Do you want a cup of coffee?', '¿Quieres una taza de café?')\n",
      "Preprocessed: ('<start> Do you want a cup of coffee ? <end>', '<start> ¿ Quieres una taza de cafe ? <end>')\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\", sentences[0])\n",
    "sentences = [(preprocess(source), preprocess(target)) for (source, target) in sentences]\n",
    "print(\"Preprocessed:\", sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('<start> Do you want a cup of coffee ? <end>',\n",
       "  '<start> I ve had coffee already . <end>',\n",
       "  '<start> Can I get you a coffee ? <end>',\n",
       "  '<start> Please give me some coffee . <end>',\n",
       "  '<start> Would you like me to make coffee ? <end>',\n",
       "  '<start> Two coffees , please . <end>',\n",
       "  '<start> How about a cup of coffee ? <end>',\n",
       "  '<start> I drank two cups of coffee . <end>',\n",
       "  '<start> Would you like to have a cup of coffee ? <end>',\n",
       "  '<start> There ll be coffee and cake at five . <end>',\n",
       "  '<start> Another coffee , please . <end>',\n",
       "  '<start> I made coffee . <end>',\n",
       "  '<start> I would like to have a cup of coffee . <end>',\n",
       "  '<start> Do you want me to make coffee ? <end>',\n",
       "  '<start> It is hard to wake up without a strong cup of coffee . <end>',\n",
       "  '<start> All I drank was coffee . <end>',\n",
       "  '<start> I ve drunk way too much coffee today . <end>',\n",
       "  '<start> Which do you prefer , tea or coffee ? <end>',\n",
       "  '<start> There are many kinds of coffee . <end>',\n",
       "  '<start> I will make some coffee . <end>'),\n",
       " ('<start> ¿ Quieres una taza de cafe ? <end>',\n",
       "  '<start> Ya tome cafe . <end>',\n",
       "  '<start> ¿ Quieres que te traiga un cafe ? <end>',\n",
       "  '<start> Dame algo de cafe por favor . <end>',\n",
       "  '<start> ¿ Quieres que prepare cafe ? <end>',\n",
       "  '<start> Dos cafes , por favor . <end>',\n",
       "  '<start> ¿ Que tal una taza de cafe ? <end>',\n",
       "  '<start> Me tome dos tazas de cafe . <end>',\n",
       "  '<start> ¿ Te gustaria tomar una taza de cafe ? <end>',\n",
       "  '<start> A las cinco habra cafe y un pastel . <end>',\n",
       "  '<start> Otro cafe , por favor . <end>',\n",
       "  '<start> Hice cafe . <end>',\n",
       "  '<start> Quiero beber una taza de cafe . <end>',\n",
       "  '<start> ¿ Quieres que haga cafe ? <end>',\n",
       "  '<start> Es dificil despertarse sin una taza de cafe fuerte . <end>',\n",
       "  '<start> Todo lo que bebi fue cafe . <end>',\n",
       "  '<start> He bebido demasiado cafe hoy . <end>',\n",
       "  '<start> ¿ Que prefieres , te o cafe ? <end>',\n",
       "  '<start> Hay muchas variedades de cafe . <end>',\n",
       "  '<start> Preparare algo de cafe . <end>'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_sentences, target_sentences = list(zip(*sentences))\n",
    "source_sentences, target_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: [1, 12, 8, 19, 9, 10, 6, 3, 7, 2]\n",
      "Sequence: [ 1 12  8 19  9 10  6  3  7  2  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "source_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\" \")\n",
    "source_tokenizer.fit_on_texts(source_sentences)\n",
    "source_data=source_tokenizer.texts_to_sequences(source_sentences)\n",
    "print(\"Sequence:\", source_data[0])\n",
    "source_data=tf.keras.preprocessing.sequence.pad_sequences(source_data, padding=\"post\")\n",
    "print(\"Sequence:\", source_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "target_tokenizer.fit_on_texts(target_sentences)\n",
    "target_data = target_tokenizer.texts_to_sequences(target_sentences)\n",
    "target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for the decoder by shifting the target sequence\n",
    "# one to the right.\n",
    "#target_data.shape\n",
    "target_labels = np.zeros(target_data.shape)\n",
    "target_labels[:,0:target_data.shape[1] -1] = target_data[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sequence [ 1  6 11  9 10  5  3  7  2  0  0  0]\n",
      "Target label [ 6. 11.  9. 10.  5.  3.  7.  2.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Target sequence\", target_data[0])\n",
    "print(\"Target label\", target_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> <start>\n",
      "12 -> do\n",
      "8 -> you\n",
      "19 -> want\n",
      "9 -> a\n",
      "10 -> cup\n",
      "6 -> of\n",
      "3 -> coffee\n",
      "7 -> ?\n",
      "2 -> <end>\n"
     ]
    }
   ],
   "source": [
    "def decode(encoded, tokenizer):\n",
    "  for number in encoded:\n",
    "    if number !=0:\n",
    "      print (\"%d -> %s\" % (number, tokenizer.index_word[number]))\n",
    "      \n",
    "decode(source_data[0], source_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices((source_data, target_data, target_labels)).batch(batch_size)\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "rnn_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Encoder, self).__init__()\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(source_vocab_size,\n",
    "                                               embedding_size)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True)\n",
    "    \n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state=hidden)        \n",
    "    return output, state\n",
    "  \n",
    "  def init_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, rnn_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15)\n"
     ]
    }
   ],
   "source": [
    "ex_sentence = tf.expand_dims(source_data[0], axis=0)\n",
    "ex_sentence\n",
    "ex_translation = tf.expand_dims(target_data[0], axis=0)\n",
    "ex_translation\n",
    "ex_labels = tf.expand_dims(target_labels[0], axis=0)\n",
    "print(ex_sentence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64)\n",
      "(1, 15, 64)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder()\n",
    "hidden_state = encoder.init_state(batch_size=1)\n",
    "print(hidden_state.shape)\n",
    "\n",
    "output, hidden_state = encoder(ex_sentence, hidden_state)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, \n",
    "                                               embedding_size)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "    logits = self.dense(output)\n",
    "    return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(idx=None):\n",
    "  \n",
    "    if idx == None: \n",
    "      idx = np.random.choice(len(sentences))\n",
    "    \n",
    "    input_sent = source_data[idx]\n",
    "    input_sent = tf.expand_dims(input_sent, axis=0)\n",
    "    \n",
    "    hidden_state = encoder.init_state(batch_size=1)\n",
    "    output, hidden_state = encoder(input_sent, hidden_state)\n",
    "    \n",
    "    decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
    "    out_words = []\n",
    "    \n",
    "    decoder_state = hidden_state\n",
    "\n",
    "    while True:\n",
    "      \n",
    "        decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
    "        decoder_input = tf.argmax(decoder_output, -1)\n",
    "        word_idx = decoder_input.numpy()[0][0]\n",
    "        # if we've predicted 0 (which is reserved, usually this will only happen\n",
    "        # before the decoder is trained, just stop translating and return\n",
    "        # what we have)\n",
    "        if word_idx == 0: \n",
    "          out_words.append('<end>')\n",
    "        else:\n",
    "          out_words.append(target_tokenizer.index_word[word_idx])\n",
    "\n",
    "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
    "          break\n",
    "          \n",
    "    translation = ' '.join(out_words)    \n",
    "    return sentences[idx][0], sentences[idx][1], translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-67fcb03f8bf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput_sent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_sent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-e6ee980905a1>\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mdecoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mdecoder_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mword_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m# if we've predicted 0 (which is reserved, usually this will only happen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# before the decoder is trained, just stop translating and return\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "input_sent, target_sent, translation = translate()\n",
    "print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
