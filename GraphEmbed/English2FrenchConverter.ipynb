{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in c:\\users\\masudrahman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: portalocker in c:\\users\\masudrahman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sacrebleu) (1.5.2)\n",
      "Requirement already satisfied: typing in c:\\users\\masudrahman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sacrebleu) (3.7.4.1)\n",
      "Requirement already satisfied: pywin32!=226; platform_system == \"Windows\" in c:\\users\\masudrahman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from portalocker->sacrebleu) (225)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#install BLEU score implementation\n",
    "!pip install sacrebleu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Could not import signal.SIGPIPE (this is expected on Windows machines)\n"
     ]
    }
   ],
   "source": [
    "#import required libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import sacrebleu\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for experiments\n",
    "sentences = [\n",
    "  (\"Do you want a cup of coffee?\", \"¿Quieres una taza de café?\"),\n",
    "  (\"I've had coffee already.\", \"Ya tomé café.\"),\n",
    "  (\"Can I get you a coffee?\", \"¿Quieres que te traiga un café?\"),\n",
    "  (\"Please give me some coffee.\", \"Dame algo de café por favor.\"),\n",
    "  (\"Would you like me to make coffee?\", \"¿Quieres que prepare café?\"),\n",
    "  (\"Two coffees, please.\", \"Dos cafés, por favor.\"),\n",
    "  (\"How about a cup of coffee?\", \"¿Qué tal una taza de café?\"),\n",
    "  (\"I drank two cups of coffee.\", \"Me tomé dos tazas de café.\"),\n",
    "  (\"Would you like to have a cup of coffee?\", \"¿Te gustaría tomar una taza de café?\"),\n",
    "  (\"There'll be coffee and cake at five.\", \"A las cinco habrá café y un pastel.\"),\n",
    "  (\"Another coffee, please.\", \"Otro café, por favor.\"),\n",
    "  (\"I made coffee.\", \"Hice café.\"),\n",
    "  (\"I would like to have a cup of coffee.\", \"Quiero beber una taza de café.\"),\n",
    "  (\"Do you want me to make coffee?\", \"¿Quieres que haga café?\"),\n",
    "  (\"It is hard to wake up without a strong cup of coffee.\", \"Es difícil despertarse sin una taza de café fuerte.\"),\n",
    "  (\"All I drank was coffee.\", \"Todo lo que bebí fue café.\"),\n",
    "  (\"I've drunk way too much coffee today.\", \"He bebido demasiado café hoy.\"),\n",
    "  (\"Which do you prefer, tea or coffee?\", \"¿Qué prefieres, té o café?\"),\n",
    "  (\"There are many kinds of coffee.\", \"Hay muchas variedades de café.\"),\n",
    "  (\"I will make some coffee.\",\t\"Prepararé algo de café.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Do you want a cup of coffee?', '¿Quieres una taza de café?')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing function\n",
    "def preprocess(s):\n",
    "  # for details, see https://www.tensorflow.org/alpha/tutorials/sequences/nmt_with_attention\n",
    "  s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "  s = re.sub(r'[\" \"]+', \" \", s)\n",
    "  s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
    "  s = s.strip()\n",
    "  s = '<start> ' + s + ' <end>'\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ('Do you want a cup of coffee?', '¿Quieres una taza de café?')\n",
      "Preprocessed: ('<start> Do you want a cup of coffee ? <end>', '<start> ¿ Quieres una taza de cafe ? <end>')\n"
     ]
    }
   ],
   "source": [
    "# wrapping the sentences with tags\n",
    "print(\"Original:\", sentences[0])\n",
    "tagged_sentences = [(preprocess(source), preprocess(target)) for (source, target) in sentences]\n",
    "print(\"Preprocessed:\", tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('<start> Do you want a cup of coffee ? <end>',\n",
       "  '<start> I ve had coffee already . <end>',\n",
       "  '<start> Can I get you a coffee ? <end>',\n",
       "  '<start> Please give me some coffee . <end>',\n",
       "  '<start> Would you like me to make coffee ? <end>',\n",
       "  '<start> Two coffees , please . <end>',\n",
       "  '<start> How about a cup of coffee ? <end>',\n",
       "  '<start> I drank two cups of coffee . <end>',\n",
       "  '<start> Would you like to have a cup of coffee ? <end>',\n",
       "  '<start> There ll be coffee and cake at five . <end>',\n",
       "  '<start> Another coffee , please . <end>',\n",
       "  '<start> I made coffee . <end>',\n",
       "  '<start> I would like to have a cup of coffee . <end>',\n",
       "  '<start> Do you want me to make coffee ? <end>',\n",
       "  '<start> It is hard to wake up without a strong cup of coffee . <end>',\n",
       "  '<start> All I drank was coffee . <end>',\n",
       "  '<start> I ve drunk way too much coffee today . <end>',\n",
       "  '<start> Which do you prefer , tea or coffee ? <end>',\n",
       "  '<start> There are many kinds of coffee . <end>',\n",
       "  '<start> I will make some coffee . <end>'),\n",
       " ('<start> ¿ Quieres una taza de cafe ? <end>',\n",
       "  '<start> Ya tome cafe . <end>',\n",
       "  '<start> ¿ Quieres que te traiga un cafe ? <end>',\n",
       "  '<start> Dame algo de cafe por favor . <end>',\n",
       "  '<start> ¿ Quieres que prepare cafe ? <end>',\n",
       "  '<start> Dos cafes , por favor . <end>',\n",
       "  '<start> ¿ Que tal una taza de cafe ? <end>',\n",
       "  '<start> Me tome dos tazas de cafe . <end>',\n",
       "  '<start> ¿ Te gustaria tomar una taza de cafe ? <end>',\n",
       "  '<start> A las cinco habra cafe y un pastel . <end>',\n",
       "  '<start> Otro cafe , por favor . <end>',\n",
       "  '<start> Hice cafe . <end>',\n",
       "  '<start> Quiero beber una taza de cafe . <end>',\n",
       "  '<start> ¿ Quieres que haga cafe ? <end>',\n",
       "  '<start> Es dificil despertarse sin una taza de cafe fuerte . <end>',\n",
       "  '<start> Todo lo que bebi fue cafe . <end>',\n",
       "  '<start> He bebido demasiado cafe hoy . <end>',\n",
       "  '<start> ¿ Que prefieres , te o cafe ? <end>',\n",
       "  '<start> Hay muchas variedades de cafe . <end>',\n",
       "  '<start> Preparare algo de cafe . <end>'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source target sentence pairs\n",
    "source_sentences, target_sentences = list(zip(*tagged_sentences))\n",
    "source_sentences, target_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: [1, 12, 8, 19, 9, 10, 6, 3, 7, 2]\n",
      "Sequence: [ 1 12  8 19  9 10  6  3  7  2  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the source sentences\n",
    "source_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\" \")\n",
    "source_tokenizer.fit_on_texts(source_sentences)\n",
    "source_data=source_tokenizer.texts_to_sequences(source_sentences)\n",
    "print(\"Sequence:\", source_data[0])\n",
    "source_data=tf.keras.preprocessing.sequence.pad_sequences(source_data, padding=\"post\")\n",
    "print(\"Sequence:\", source_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: [ 1  6 11  9 10  5  3  7  2  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the target sentences\n",
    "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\" \")\n",
    "target_tokenizer.fit_on_texts(target_sentences)\n",
    "target_data = target_tokenizer.texts_to_sequences(target_sentences)\n",
    "target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, padding='post')\n",
    "print(\"Sequence:\", target_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 15)\n"
     ]
    }
   ],
   "source": [
    "print(source_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  6 11  9 10  5  3  7  2  0  0  0]\n",
      " [ 1 20 16  3  4  2  0  0  0  0  0  0]\n",
      " [ 1  6 11  8 12 21 17  3  7  2  0  0]\n",
      " [ 1 22 18  5  3 13 14  4  2  0  0  0]\n",
      " [ 1  6 11  8 23  3  7  2  0  0  0  0]\n",
      " [ 1 19 24 15 13 14  4  2  0  0  0  0]\n",
      " [ 1  6  8 25  9 10  5  3  7  2  0  0]\n",
      " [ 1 26 16 19 27  5  3  4  2  0  0  0]\n",
      " [ 1  6 12 28 29  9 10  5  3  7  2  0]\n",
      " [ 1 30 31 32 33  3 34 17 35  4  2  0]\n",
      " [ 1 36  3 15 13 14  4  2  0  0  0  0]\n",
      " [ 1 37  3  4  2  0  0  0  0  0  0  0]\n",
      " [ 1 38 39  9 10  5  3  4  2  0  0  0]\n",
      " [ 1  6 11  8 40  3  7  2  0  0  0  0]\n",
      " [ 1 41 42 43 44  9 10  5  3 45  4  2]\n",
      " [ 1 46 47  8 48 49  3  4  2  0  0  0]\n",
      " [ 1 50 51 52  3 53  4  2  0  0  0  0]\n",
      " [ 1  6  8 54 15 12 55  3  7  2  0  0]\n",
      " [ 1 56 57 58  5  3  4  2  0  0  0  0]\n",
      " [ 1 59 18  5  3  4  2  0  0  0  0  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(target_data)\n",
    "target_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6. 11.  9. 10.  5.  3.  7.  2.  0.  0.  0.  0.]\n",
      " [20. 16.  3.  4.  2.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 6. 11.  8. 12. 21. 17.  3.  7.  2.  0.  0.  0.]\n",
      " [22. 18.  5.  3. 13. 14.  4.  2.  0.  0.  0.  0.]\n",
      " [ 6. 11.  8. 23.  3.  7.  2.  0.  0.  0.  0.  0.]\n",
      " [19. 24. 15. 13. 14.  4.  2.  0.  0.  0.  0.  0.]\n",
      " [ 6.  8. 25.  9. 10.  5.  3.  7.  2.  0.  0.  0.]\n",
      " [26. 16. 19. 27.  5.  3.  4.  2.  0.  0.  0.  0.]\n",
      " [ 6. 12. 28. 29.  9. 10.  5.  3.  7.  2.  0.  0.]\n",
      " [30. 31. 32. 33.  3. 34. 17. 35.  4.  2.  0.  0.]\n",
      " [36.  3. 15. 13. 14.  4.  2.  0.  0.  0.  0.  0.]\n",
      " [37.  3.  4.  2.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [38. 39.  9. 10.  5.  3.  4.  2.  0.  0.  0.  0.]\n",
      " [ 6. 11.  8. 40.  3.  7.  2.  0.  0.  0.  0.  0.]\n",
      " [41. 42. 43. 44.  9. 10.  5.  3. 45.  4.  2.  0.]\n",
      " [46. 47.  8. 48. 49.  3.  4.  2.  0.  0.  0.  0.]\n",
      " [50. 51. 52.  3. 53.  4.  2.  0.  0.  0.  0.  0.]\n",
      " [ 6.  8. 54. 15. 12. 55.  3.  7.  2.  0.  0.  0.]\n",
      " [56. 57. 58.  5.  3.  4.  2.  0.  0.  0.  0.  0.]\n",
      " [59. 18.  5.  3.  4.  2.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#creating the class labels by shifting one keyword\n",
    "target_labels = np.zeros(target_data.shape)\n",
    "#print(target_labels)\n",
    "target_labels[:,0:target_data.shape[1] -1] = target_data[:,1:]\n",
    "print(target_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sequence [ 1  6 11  9 10  5  3  7  2  0  0  0]\n",
      "Target label [ 6. 11.  9. 10.  5.  3.  7.  2.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Target sequence\", target_data[0])\n",
    "print(\"Target label\", target_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 60)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate vocabulary size\n",
    "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "source_vocab_size,target_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define decode function\n",
    "\n",
    "def decode(encoded, tokenizer):\n",
    "  for number in encoded:\n",
    "    if number !=0:\n",
    "      print (\"%d -> %s\" % (number, tokenizer.index_word[number]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> <start>\n",
      "12 -> do\n",
      "8 -> you\n",
      "19 -> want\n",
      "9 -> a\n",
      "10 -> cup\n",
      "6 -> of\n",
      "3 -> coffee\n",
      "7 -> ?\n",
      "2 -> <end>\n"
     ]
    }
   ],
   "source": [
    "decode(source_data[0], source_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "3\n",
      "0\n",
      "8\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
    "for elem in dataset:\n",
    "  print(elem.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=41, shape=(5, 15), dtype=int32, numpy=\n",
      "array([[ 1, 12,  8, 19,  9, 10,  6,  3,  7,  2,  0,  0,  0,  0,  0],\n",
      "       [ 1,  5, 20, 26,  3, 27,  4,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1, 28,  5, 29,  8,  9,  3,  7,  2,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1, 13, 30, 14, 21,  3,  4,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1, 15,  8, 16, 14, 11, 17,  3,  7,  2,  0,  0,  0,  0,  0]])>, <tf.Tensor: id=42, shape=(5, 12), dtype=int32, numpy=\n",
      "array([[ 1,  6, 11,  9, 10,  5,  3,  7,  2,  0,  0,  0],\n",
      "       [ 1, 20, 16,  3,  4,  2,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  6, 11,  8, 12, 21, 17,  3,  7,  2,  0,  0],\n",
      "       [ 1, 22, 18,  5,  3, 13, 14,  4,  2,  0,  0,  0],\n",
      "       [ 1,  6, 11,  8, 23,  3,  7,  2,  0,  0,  0,  0]])>, <tf.Tensor: id=43, shape=(5, 12), dtype=float64, numpy=\n",
      "array([[ 6., 11.,  9., 10.,  5.,  3.,  7.,  2.,  0.,  0.,  0.,  0.],\n",
      "       [20., 16.,  3.,  4.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 6., 11.,  8., 12., 21., 17.,  3.,  7.,  2.,  0.,  0.,  0.],\n",
      "       [22., 18.,  5.,  3., 13., 14.,  4.,  2.,  0.,  0.,  0.,  0.],\n",
      "       [ 6., 11.,  8., 23.,  3.,  7.,  2.,  0.,  0.,  0.,  0.,  0.]])>)\n",
      "(<tf.Tensor: id=44, shape=(5, 15), dtype=int32, numpy=\n",
      "array([[ 1, 22, 31, 18, 13,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1, 32, 33,  9, 10,  6,  3,  7,  2,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  5, 23, 22, 34,  6,  3,  4,  2,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1, 15,  8, 16, 11, 24,  9, 10,  6,  3,  7,  2,  0,  0,  0],\n",
      "       [ 1, 25, 35, 36,  3, 37, 38, 39, 40,  4,  2,  0,  0,  0,  0]])>, <tf.Tensor: id=45, shape=(5, 12), dtype=int32, numpy=\n",
      "array([[ 1, 19, 24, 15, 13, 14,  4,  2,  0,  0,  0,  0],\n",
      "       [ 1,  6,  8, 25,  9, 10,  5,  3,  7,  2,  0,  0],\n",
      "       [ 1, 26, 16, 19, 27,  5,  3,  4,  2,  0,  0,  0],\n",
      "       [ 1,  6, 12, 28, 29,  9, 10,  5,  3,  7,  2,  0],\n",
      "       [ 1, 30, 31, 32, 33,  3, 34, 17, 35,  4,  2,  0]])>, <tf.Tensor: id=46, shape=(5, 12), dtype=float64, numpy=\n",
      "array([[19., 24., 15., 13., 14.,  4.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 6.,  8., 25.,  9., 10.,  5.,  3.,  7.,  2.,  0.,  0.,  0.],\n",
      "       [26., 16., 19., 27.,  5.,  3.,  4.,  2.,  0.,  0.,  0.,  0.],\n",
      "       [ 6., 12., 28., 29.,  9., 10.,  5.,  3.,  7.,  2.,  0.,  0.],\n",
      "       [30., 31., 32., 33.,  3., 34., 17., 35.,  4.,  2.,  0.,  0.]])>)\n",
      "(<tf.Tensor: id=47, shape=(5, 15), dtype=int32, numpy=\n",
      "array([[ 1, 41,  3, 18, 13,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  5, 42,  3,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  5, 15, 16, 11, 24,  9, 10,  6,  3,  4,  2,  0,  0,  0],\n",
      "       [ 1, 12,  8, 19, 14, 11, 17,  3,  7,  2,  0,  0,  0,  0,  0],\n",
      "       [ 1, 43, 44, 45, 11, 46, 47, 48,  9, 49, 10,  6,  3,  4,  2]])>, <tf.Tensor: id=48, shape=(5, 12), dtype=int32, numpy=\n",
      "array([[ 1, 36,  3, 15, 13, 14,  4,  2,  0,  0,  0,  0],\n",
      "       [ 1, 37,  3,  4,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1, 38, 39,  9, 10,  5,  3,  4,  2,  0,  0,  0],\n",
      "       [ 1,  6, 11,  8, 40,  3,  7,  2,  0,  0,  0,  0],\n",
      "       [ 1, 41, 42, 43, 44,  9, 10,  5,  3, 45,  4,  2]])>, <tf.Tensor: id=49, shape=(5, 12), dtype=float64, numpy=\n",
      "array([[36.,  3., 15., 13., 14.,  4.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [37.,  3.,  4.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [38., 39.,  9., 10.,  5.,  3.,  4.,  2.,  0.,  0.,  0.,  0.],\n",
      "       [ 6., 11.,  8., 40.,  3.,  7.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [41., 42., 43., 44.,  9., 10.,  5.,  3., 45.,  4.,  2.,  0.]])>)\n",
      "(<tf.Tensor: id=50, shape=(5, 15), dtype=int32, numpy=\n",
      "array([[ 1, 50,  5, 23, 51,  3,  4,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  5, 20, 52, 53, 54, 55,  3, 56,  4,  2,  0,  0,  0,  0],\n",
      "       [ 1, 57, 12,  8, 58, 18, 59, 60,  3,  7,  2,  0,  0,  0,  0],\n",
      "       [ 1, 25, 61, 62, 63,  6,  3,  4,  2,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  5, 64, 17, 21,  3,  4,  2,  0,  0,  0,  0,  0,  0,  0]])>, <tf.Tensor: id=51, shape=(5, 12), dtype=int32, numpy=\n",
      "array([[ 1, 46, 47,  8, 48, 49,  3,  4,  2,  0,  0,  0],\n",
      "       [ 1, 50, 51, 52,  3, 53,  4,  2,  0,  0,  0,  0],\n",
      "       [ 1,  6,  8, 54, 15, 12, 55,  3,  7,  2,  0,  0],\n",
      "       [ 1, 56, 57, 58,  5,  3,  4,  2,  0,  0,  0,  0],\n",
      "       [ 1, 59, 18,  5,  3,  4,  2,  0,  0,  0,  0,  0]])>, <tf.Tensor: id=52, shape=(5, 12), dtype=float64, numpy=\n",
      "array([[46., 47.,  8., 48., 49.,  3.,  4.,  2.,  0.,  0.,  0.,  0.],\n",
      "       [50., 51., 52.,  3., 53.,  4.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 6.,  8., 54., 15., 12., 55.,  3.,  7.,  2.,  0.,  0.,  0.],\n",
      "       [56., 57., 58.,  5.,  3.,  4.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [59., 18.,  5.,  3.,  4.,  2.,  0.,  0.,  0.,  0.,  0.,  0.]])>)\n"
     ]
    }
   ],
   "source": [
    "# define the dataset with batch size\n",
    "batch_size = 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices((source_data, target_data, target_labels)).batch(batch_size)\n",
    "for elem in dataset:\n",
    "  print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration data\n",
    "embedding_size = 32\n",
    "rnn_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define encoder class\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Encoder, self).__init__()\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(source_vocab_size,\n",
    "                                               embedding_size)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True)\n",
    "    \n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state=hidden)        \n",
    "    return output, state\n",
    "  \n",
    "  def init_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, rnn_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15) (1, 12) (1, 12)\n"
     ]
    }
   ],
   "source": [
    "ex_sentence = tf.expand_dims(source_data[0], axis=0)\n",
    "ex_sentence\n",
    "ex_translation = tf.expand_dims(target_data[0], axis=0)\n",
    "ex_translation\n",
    "ex_labels = tf.expand_dims(target_labels[0], axis=0)\n",
    "print(ex_sentence.shape, ex_translation.shape, ex_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder()\n",
    "hidden_state = encoder.init_state(batch_size=1)\n",
    "print(hidden_state.shape)\n",
    "\n",
    "output, hidden_state = encoder(ex_sentence, hidden_state)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the decoder class\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, \n",
    "                                               embedding_size)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "    logits = self.dense(output)\n",
    "    return logits, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 1 12  8 19  9 10  6  3  7  2  0  0  0  0  0]], shape=(1, 15), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#debugging of the constructed items\n",
    "#debugging of items\n",
    "input_sent = source_data[0]\n",
    "input_sent = tf.expand_dims(input_sent, axis=0)\n",
    "print(input_sent)\n",
    "\n",
    "hidden_state = encoder.init_state(batch_size=1)\n",
    "output, hidden_state = encoder(input_sent, hidden_state)\n",
    "#print(output, hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']], axis=0)\n",
    "print(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.00914953  0.00451236 -0.00965445  0.00386949  0.00790744 -0.01609711\n",
      "   0.02677082 -0.01060981  0.00955295  0.01818538  0.01540457  0.01056781\n",
      "   0.00658734  0.02966356 -0.01944757 -0.00051034  0.02631384  0.0165773\n",
      "  -0.01260577  0.02279333  0.0023177   0.01755016 -0.01033975  0.02802606\n",
      "   0.01639891 -0.00434892  0.01363721 -0.02372819  0.01855787 -0.0046172\n",
      "   0.01304835 -0.00377202  0.01229745 -0.00640432  0.02098239  0.00826915\n",
      "  -0.00606747  0.01547022  0.00860626 -0.00881946  0.01471723 -0.01078211\n",
      "  -0.00271665  0.00400213 -0.00543378  0.02301455 -0.00844434 -0.00918204\n",
      "   0.01584662  0.02288154  0.0114758  -0.00512738  0.0038305   0.01713556\n",
      "   0.02562306  0.00689743 -0.00112571  0.01507663  0.0413456   0.03438532\n",
      "  -0.00144762 -0.01492808 -0.00288165  0.00951597]], shape=(1, 64), dtype=float32)\n",
      "tf.Tensor([[19]], shape=(1, 1), dtype=int64)\n",
      "dos\n"
     ]
    }
   ],
   "source": [
    "decoder_state=hidden_state\n",
    "print(hidden_state)\n",
    "decoder=Decoder()\n",
    "decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
    "# print(decoder_output, decoder_state)\n",
    "\n",
    "decoder_input = tf.argmax(decoder_output, -1)\n",
    "word_idx = decoder_input.numpy()[0][0]\n",
    "print(decoder_input)\n",
    "print(target_tokenizer.index_word[word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the translation\n",
    "\n",
    "def translate(idx=None):\n",
    "  \n",
    "    if idx == None: \n",
    "      idx = np.random.choice(len(sentences))\n",
    "    \n",
    "    input_sent = source_data[idx]\n",
    "    input_sent = tf.expand_dims(input_sent, axis=0)\n",
    "    print(input_sent)\n",
    "    \n",
    "    hidden_state = encoder.init_state(batch_size=1)\n",
    "    output, hidden_state = encoder(input_sent, hidden_state)\n",
    "    \n",
    "    decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
    "    out_words = []\n",
    "    print(decoder_input)\n",
    "    \n",
    "    decoder_state = hidden_state\n",
    "    \n",
    "    decoder=Decoder()\n",
    "\n",
    "    while True:\n",
    "      \n",
    "        decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
    "        decoder_input = tf.argmax(decoder_output, -1)\n",
    "        word_idx = decoder_input.numpy()[0][0]\n",
    "        # if we've predicted 0 (which is reserved, usually this will only happen\n",
    "        # before the decoder is trained, just stop translating and return\n",
    "        # what we have)\n",
    "        if word_idx == 0: \n",
    "          out_words.append('<end>')\n",
    "        else:\n",
    "          out_words.append(target_tokenizer.index_word[word_idx])\n",
    "\n",
    "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
    "          break\n",
    "          \n",
    "    translation = ' '.join(out_words)    \n",
    "    return sentences[idx][0], sentences[idx][1], translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 1 32 33  9 10  6  3  7  2  0  0  0  0  0  0]], shape=(1, 15), dtype=int32)\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "Input: How about a cup of coffee?\n",
      "Target: ¿Qué tal una taza de café?\n",
      "Translation: ¿ <start> <start> habra y dame sin taza bebido y tomar o cinco lo dos fuerte hice quieres hice habra\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sent, target_sent, translation = translate()\n",
    "print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
