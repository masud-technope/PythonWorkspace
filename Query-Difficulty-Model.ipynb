{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Activation\n",
    "from keras.layers import SimpleRNN, TimeDistributed\n",
    "from sklearn import model_selection\n",
    "\n",
    "csv_data=pd.read_csv(\"data/blader-data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "# print(csv_data.describe())\n",
    "print(len(csv_data.columns))\n",
    "\n",
    "#change column values\n",
    "csv_data = csv_data.replace({'qclass': {1: 'high', 0:'low'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting features and labels\n",
    "\n",
    "all_features=csv_data.iloc[:, 0:22]\n",
    "all_labels=csv_data['qclass']\n",
    "\n",
    "encoder.fit(all_labels)\n",
    "all_labels_encoded = encoder.transform(all_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['high' 'low']\n"
     ]
    }
   ],
   "source": [
    "#training and testing split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=model_selection.train_test_split(all_features,all_labels_encoded,test_size=0.20)\n",
    "\n",
    "#X_train=csv_data.iloc[:, 0:22]\n",
    "#Y_train=csv_data['qclass']\n",
    "\n",
    "#X_train.describe()\n",
    "#Y_train.describe()\n",
    "\n",
    "input_dim=X_train.shape[1]\n",
    "\n",
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm=SMOTE()\n",
    "X_train_sampled, Y_train_sampled = sm.fit_sample(X_train, Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.transform([\"low\",\"high\",\"low\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 64)                1472      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,602\n",
      "Trainable params: 1,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=2)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#input_dim=X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64,input_shape=(input_dim,)))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(output_dim=2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "one_hot_labels = keras.utils.to_categorical(Y_train, num_classes=2)\n",
    "print(one_hot_labels)\n",
    "\n",
    "test_one_hot_labels=keras.utils.to_categorical(Y_test,num_classes=2)\n",
    "print(test_one_hot_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot_labels = keras.utils.to_categorical(Y_train_sampled, num_classes=2)\n",
    "print(one_hot_labels)\n",
    "\n",
    "test_one_hot_labels=keras.utils.to_categorical(Y_test,num_classes=2)\n",
    "print(test_one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: imbalanced-learn in c:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11 in c:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages (from imbalanced-learn) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17 in c:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages (from imbalanced-learn) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21 in c:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages (from imbalanced-learn) (0.21.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages (from imbalanced-learn) (0.13.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'sampler' needs to have an attribute 'sample_indices_'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-4553ffd9d1a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtraining_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbalanced_batch_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages\\imblearn\\keras\\_generator.py\u001b[0m in \u001b[0;36mbalanced_batch_generator\u001b[1;34m(X, y, sample_weight, sampler, batch_size, keep_sparse, random_state)\u001b[0m\n\u001b[0;32m    264\u001b[0m     return tf_bbg(X=X, y=y, sample_weight=sample_weight,\n\u001b[0;32m    265\u001b[0m                   \u001b[0msampler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m                   keep_sparse=keep_sparse, random_state=random_state)\n\u001b[0m",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages\\imblearn\\tensorflow\\_generator.py\u001b[0m in \u001b[0;36mbalanced_batch_generator\u001b[1;34m(X, y, sample_weight, sampler, batch_size, keep_sparse, random_state)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0msampler_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampler_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sample_indices_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         raise ValueError(\"'sampler' needs to have an attribute \"\n\u001b[0m\u001b[0;32m    132\u001b[0m                          \"'sample_indices_'.\")\n\u001b[0;32m    133\u001b[0m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampler_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_indices_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'sampler' needs to have an attribute 'sample_indices_'."
     ]
    }
   ],
   "source": [
    "#creating a balanced batch\n",
    "\n",
    "\n",
    "#system command to install imblearn\n",
    "# !pip install -U imbalanced-learn\n",
    "\n",
    "from imblearn.keras import balanced_batch_generator\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_generator, steps_per_epoch = balanced_batch_generator(X_train, one_hot_labels, sampler=SMOTE(), batch_size=10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 1s - loss: 0.8047 - acc: 0.4870\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.7082 - acc: 0.5130\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.6981 - acc: 0.5170\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.6980 - acc: 0.5212\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.6913 - acc: 0.5387\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.6927 - acc: 0.5165\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.6870 - acc: 0.5269\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.6888 - acc: 0.5297\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.6887 - acc: 0.5249\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.6936 - acc: 0.5170\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.6947 - acc: 0.5162\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.6904 - acc: 0.5224\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.6909 - acc: 0.5112\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.6883 - acc: 0.5334\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.6897 - acc: 0.5202\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.6869 - acc: 0.5307\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.6880 - acc: 0.5294\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.6854 - acc: 0.5299\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.6906 - acc: 0.5140\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.6904 - acc: 0.5377\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.6884 - acc: 0.5277\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.6883 - acc: 0.5284\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.6880 - acc: 0.5357\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.6888 - acc: 0.5262\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.6862 - acc: 0.5409\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.6874 - acc: 0.5247\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.6844 - acc: 0.5424\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.6870 - acc: 0.5337\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.6870 - acc: 0.5165\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.6906 - acc: 0.5237\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.6892 - acc: 0.5232\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.6884 - acc: 0.5239\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.6874 - acc: 0.5297\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.6891 - acc: 0.5257\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.6895 - acc: 0.5264\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.6895 - acc: 0.5190\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.6874 - acc: 0.5379\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.6878 - acc: 0.5327\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.6869 - acc: 0.5262\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.6887 - acc: 0.5180\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.6875 - acc: 0.5299\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.6875 - acc: 0.5202\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.6880 - acc: 0.5307\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.6926 - acc: 0.5062\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.6885 - acc: 0.5195\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.6896 - acc: 0.5284\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.6878 - acc: 0.5297\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.6900 - acc: 0.5182\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.6889 - acc: 0.5259\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.6924 - acc: 0.5062\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.6856 - acc: 0.5352\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.6872 - acc: 0.5232\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.6873 - acc: 0.5329\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.6885 - acc: 0.5279\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.6900 - acc: 0.5212\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.6875 - acc: 0.5352\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.6883 - acc: 0.5167\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.6886 - acc: 0.5187\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.6886 - acc: 0.5170\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.6905 - acc: 0.5047\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.6884 - acc: 0.5262\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.6879 - acc: 0.5207\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.6891 - acc: 0.5342\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.6885 - acc: 0.5155\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.6892 - acc: 0.5237\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.6894 - acc: 0.5267\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.6890 - acc: 0.5080\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.6895 - acc: 0.5232\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.6885 - acc: 0.5254\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.6878 - acc: 0.5152\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.6881 - acc: 0.5274\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.6862 - acc: 0.5239\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.6881 - acc: 0.5242\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.6869 - acc: 0.5232\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.6863 - acc: 0.5224\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.6865 - acc: 0.5252\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.6880 - acc: 0.5125\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.6887 - acc: 0.5277\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.6901 - acc: 0.5160\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.6872 - acc: 0.5272\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.6871 - acc: 0.5167\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.6860 - acc: 0.5317\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.6847 - acc: 0.5282\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.6862 - acc: 0.5334\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.6883 - acc: 0.5195\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.6870 - acc: 0.5237\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.6853 - acc: 0.5324\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.6843 - acc: 0.5401\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.6847 - acc: 0.5384\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.6845 - acc: 0.5387\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.6852 - acc: 0.5436\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.6857 - acc: 0.5389\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.6850 - acc: 0.5337\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.6841 - acc: 0.5446\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.6876 - acc: 0.5212\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.6843 - acc: 0.5357\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.6850 - acc: 0.5389\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.6856 - acc: 0.5302\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.6845 - acc: 0.5362\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.6838 - acc: 0.5359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25b0b6cf438>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_sampled, one_hot_labels, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_10 to have shape (2,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-df2a208db82f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_sampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_sampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\graphembed\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected activation_10 to have shape (2,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model.fit(X_sampled, Y_sampled, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 1s - loss: 0.6974 - acc: 0.5252\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.6946 - acc: 0.5272\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.6922 - acc: 0.5177\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.6909 - acc: 0.5337\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.6945 - acc: 0.5172\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.6955 - acc: 0.5257\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.6940 - acc: 0.5297\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.6927 - acc: 0.5327\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.6962 - acc: 0.5100\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.6962 - acc: 0.5077\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.6944 - acc: 0.5087\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.6944 - acc: 0.5130\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.6943 - acc: 0.5214\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.6949 - acc: 0.5132\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.6963 - acc: 0.5090\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.6954 - acc: 0.5140\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.6946 - acc: 0.5127\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.6963 - acc: 0.5117\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.6973 - acc: 0.5100\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.6958 - acc: 0.5190\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.6958 - acc: 0.5170\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.6969 - acc: 0.5155\n",
      "Epoch 23/100\n",
      " - 1s - loss: 0.6965 - acc: 0.5170\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.6956 - acc: 0.5090\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.6947 - acc: 0.5167\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.6946 - acc: 0.5170\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.6933 - acc: 0.5207\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.6941 - acc: 0.5140\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.6968 - acc: 0.5107\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.6947 - acc: 0.5202\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.6978 - acc: 0.5080\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.6955 - acc: 0.5065\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.6972 - acc: 0.5062\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.6970 - acc: 0.5032\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.6960 - acc: 0.5067\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.6961 - acc: 0.5032\n",
      "Epoch 37/100\n",
      " - 1s - loss: 0.6943 - acc: 0.5229\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.6943 - acc: 0.5227\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.6952 - acc: 0.5172\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.6935 - acc: 0.5195\n",
      "Epoch 41/100\n",
      " - 2s - loss: 0.6943 - acc: 0.5207\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.6964 - acc: 0.5137\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.6950 - acc: 0.5165\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.6944 - acc: 0.5170\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.6934 - acc: 0.5227\n",
      "Epoch 46/100\n",
      " - 1s - loss: 0.6927 - acc: 0.5247\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.6942 - acc: 0.5177\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.6964 - acc: 0.5127\n",
      "Epoch 49/100\n",
      " - 1s - loss: 0.6945 - acc: 0.5085\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.6943 - acc: 0.5180\n",
      "Epoch 51/100\n",
      " - 1s - loss: 0.6933 - acc: 0.5190\n",
      "Epoch 52/100\n",
      " - 1s - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 53/100\n",
      " - 1s - loss: 0.6941 - acc: 0.5227\n",
      "Epoch 54/100\n",
      " - 1s - loss: 0.6950 - acc: 0.5095\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.6941 - acc: 0.5137\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.6932 - acc: 0.5200\n",
      "Epoch 57/100\n",
      " - 1s - loss: 0.6919 - acc: 0.5195\n",
      "Epoch 58/100\n",
      " - 1s - loss: 0.6919 - acc: 0.5197\n",
      "Epoch 59/100\n",
      " - 1s - loss: 0.6928 - acc: 0.5157\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.6916 - acc: 0.5177\n",
      "Epoch 61/100\n",
      " - 1s - loss: 0.6930 - acc: 0.5160\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.6945 - acc: 0.5105\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.6918 - acc: 0.5217\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.6893 - acc: 0.5334\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.6908 - acc: 0.5264\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.6886 - acc: 0.5362\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.6892 - acc: 0.5342\n",
      "Epoch 68/100\n",
      " - 1s - loss: 0.6904 - acc: 0.5304\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.6928 - acc: 0.5289\n",
      "Epoch 70/100\n",
      " - 1s - loss: 0.6914 - acc: 0.5242\n",
      "Epoch 71/100\n",
      " - 1s - loss: 0.6883 - acc: 0.5342\n",
      "Epoch 72/100\n",
      " - 1s - loss: 0.6873 - acc: 0.5372\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.6889 - acc: 0.5289\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.6914 - acc: 0.5180\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.6923 - acc: 0.5180\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.6960 - acc: 0.5130\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.6935 - acc: 0.5107\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.6898 - acc: 0.5234\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.6914 - acc: 0.5217\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.6930 - acc: 0.5160\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.6937 - acc: 0.5152\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.6926 - acc: 0.5137\n",
      "Epoch 83/100\n",
      " - 1s - loss: 0.6923 - acc: 0.5132\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.6915 - acc: 0.5190\n",
      "Epoch 85/100\n",
      " - 1s - loss: 0.6909 - acc: 0.5192\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.6921 - acc: 0.5162\n",
      "Epoch 87/100\n",
      " - 1s - loss: 0.6925 - acc: 0.5075\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.6910 - acc: 0.5157\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.6894 - acc: 0.5202\n",
      "Epoch 90/100\n",
      " - 1s - loss: 0.6949 - acc: 0.5172\n",
      "Epoch 91/100\n",
      " - 1s - loss: 0.6936 - acc: 0.5165\n",
      "Epoch 92/100\n",
      " - 1s - loss: 0.6904 - acc: 0.5307\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.6919 - acc: 0.5209\n",
      "Epoch 94/100\n",
      " - 1s - loss: 0.6907 - acc: 0.5249\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.6898 - acc: 0.5322\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.6901 - acc: 0.5244\n",
      "Epoch 97/100\n",
      " - 1s - loss: 0.6903 - acc: 0.5327\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.6912 - acc: 0.5282\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.6919 - acc: 0.5247\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.6948 - acc: 0.5197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25b09ccd048>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=training_generator,  steps_per_epoch=steps_per_epoch, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[178  12]\n",
      " [442  28]]\n",
      "0.7 0.2870967741935484\n"
     ]
    }
   ],
   "source": [
    "# model.predict(X_test)\n",
    "#scores = model.evaluate(X_test, test_one_hot_labels, verbose=0)\n",
    "\n",
    "predictions=model.predict(X_test)\n",
    "\n",
    "#print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(test_one_hot_labels.argmax(axis=1), predictions.argmax(axis=1), labels=[0,1])\n",
    "print(matrix)\n",
    "\n",
    "\n",
    "\n",
    "tn,fp,fn,tp =  matrix.ravel()\n",
    "\n",
    "acc1=tp/(tp+fp)\n",
    "acc0=tn/(tn+fn)\n",
    "\n",
    "print(acc1,acc0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    \n",
    "    # Begin CHANGES\n",
    "    fst_empty_cell = (columnwidth-3)//2 * \" \" + \"t/p\" + (columnwidth-3)//2 * \" \"\n",
    "    \n",
    "    if len(fst_empty_cell) < len(empty_cell):\n",
    "        fst_empty_cell = \" \" * (len(empty_cell) - len(fst_empty_cell)) + fst_empty_cell\n",
    "    # Print header\n",
    "    print(\"    \" + fst_empty_cell, end=\" \")\n",
    "    # End CHANGES\n",
    "    \n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "        \n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     t/p      0     1 \n",
      "        0 178.0  12.0 \n",
      "        1 442.0  28.0 \n"
     ]
    }
   ],
   "source": [
    "print_cm(matrix,['0','1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(test_one_hot_labels)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(matrix, cmap='binary', interpolation='None')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=660, minmax=(0, 1), mean=0.7121212121212122, variance=0.20531567572538742, skewness=-0.9369842739743568, kurtosis=-1.1220604703247476)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "stats.describe(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-50dccc07d312>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mY_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
