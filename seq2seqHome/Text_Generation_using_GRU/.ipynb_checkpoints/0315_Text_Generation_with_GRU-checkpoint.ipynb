{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation using LSTM Network\n",
    "#### ディープラーニングによる文章生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LSTM?\n",
    "  \n",
    "テキスト生成は一種の言語モデリング問題。  \n",
    "言語モデリングは、テキスト読み上げ、会話システム、テキスト要約など、多数の自然言語処理タスクの中心的な問題。    \n",
    "訓練された言語モデルは、テキストで使用されている前の一連の単語に基づいて、単語の出現の可能性を学習する。  \n",
    "言語モデルは、文字レベル、n-gramレベル、文レベル、または段落レベルでも操作できる。  \n",
    "このノートでは、最先端のリカレントニューラルネットワークを実装してトレーニングすることによって、自然言語テキストを生成するための言語モデルを作成する方法について説明していく\n",
    "。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 今回はニュースの本文からタイトルを自動生成します  \n",
    "  \n",
    "#### Process  \n",
    "1. データの準備  \n",
    "2. 文章のお掃除（記号削除、小文字統一）  \n",
    "3. 単語に切り分ける  \n",
    "4. トークン化＝数値化  \n",
    "5. パディングで変数の長さを統一  \n",
    "6. LSTMの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the libraries ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset データのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\MyWorks\\PythonWorkspace\\seq2seqHome\\Text_Generation_using_GRU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is E050-A3EE\n",
      "\n",
      " Directory of C:\\MyWorks\\PythonWorkspace\\seq2seqHome\\Text_Generation_using_GRU\n",
      "\n",
      "2020-02-07  09:31 PM    <DIR>          .\n",
      "2020-02-07  09:31 PM    <DIR>          ..\n",
      "2020-02-07  09:23 PM    <DIR>          .ipynb_checkpoints\n",
      "2020-02-07  09:31 PM            16,100 0315_Text_Generation_with_GRU.ipynb\n",
      "2020-02-07  09:18 PM               742 attention.py\n",
      "2020-02-07  09:18 PM             4,632 lstm.py\n",
      "2020-02-07  09:18 PM             4,626 my_lstm.py\n",
      "2020-02-07  09:24 PM               555 Neural-Text-Generation.ipynb\n",
      "2019-10-01  06:05 PM        83,917,554 News_Category_Dataset_v2.json\n",
      "2020-02-07  09:25 PM        26,677,036 News_Category_Dataset_v2.json.zip\n",
      "2020-02-07  09:18 PM             2,718 README.md\n",
      "2020-02-07  09:18 PM            23,413 RECENT_Notenool.ipynb\n",
      "2020-02-07  09:18 PM               744 seq2seq_attention.py\n",
      "2020-02-07  09:18 PM            14,238 slack_model.py\n",
      "2020-02-07  09:22 PM           179,696 Text_Generation_with_GRU.ipynb\n",
      "              12 File(s)    110,842,054 bytes\n",
      "               3 Dir(s)  776,853,475,328 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import pandas as pd\n",
    "import json\n",
    "my_json_file=path+\"/news-category-small.json\"\n",
    "data = [json.loads(line) for line in open(my_json_file, 'r')]\n",
    "print(data[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading the headlines\n",
    "import json  \n",
    "from pandas.io.json import json_normalize  \n",
    "\n",
    "#with open(path +\"/News_Category_Dataset_v2.json\") as f: \n",
    "# d = json.loads(f) \n",
    "df=pd.read_json(path +\"/News_Category_Dataset_v2.json\")          \n",
    "print(df)\n",
    "#headlines= json_normalize(d[\"headline\"].value()) \n",
    "#print(headlines.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "['There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV', \"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\", 'Hugh Grant Marries For The First Time At Age 57', \"Jim Carrey Blasts 'Castrato' Adam Schiff And Democrats In New Artwork\", 'Julianna Margulies Uses Donald Trump Poop Bags To Pick Up After Her Dog', \"Morgan Freeman 'Devastated' That Sexual Harassment Claims Could Undermine Legacy\", \"Donald Trump Is Lovin' New McDonald's Jingle In 'Tonight Show' Bit\", 'What To Watch On Amazon Prime That’s New This Week', \"Mike Myers Reveals He'd 'Like To' Do A Fourth Austin Powers Film\", 'What To Watch On Hulu That’s New This Week', 'Justin Timberlake Visits Texas School Shooting Victims', \"South Korean President Meets North Korea's Kim Jong Un To Talk Trump Summit\", 'With Its Way Of Life At Risk, This Remote Oyster-Growing Region Called In Robots', \"Trump's Crackdown On Immigrant Parents Puts More Kids In An Already Strained System\", \"'Trump's Son Should Be Concerned': FBI Obtained Wiretaps Of Putin Ally Who Met With Trump Jr.\", \"Edward Snowden: There's No One Trump Loves More Than Vladimir Putin\", \"Booyah: Obama Photographer Hilariously Trolls Trump's 'Spy' Claim\", 'Ireland Votes To Repeal Abortion Amendment In Landslide Referendum', \"Ryan Zinke Looks To Reel Back Some Critics With 'Grand Pivot' To Conservation\", \"Trump's Scottish Golf Resort Pays Women Significantly Less Than Men: Report\", \"Weird Father's Day Gifts Your Dad Doesn't Know He Wants (But He Does)\", 'Twitter #PutStarWarsInOtherFilms And It Was Universally Entertaining', \"Mystery 'Wolf-Like' Animal Reportedly Shot In Montana, Baffles Wildlife Officials\", 'North Korea Still Open To Talks After Trump Cancels Summit', '2 Men Detonate Bomb Inside Indian Restaurant Near Toronto, Authorities Say', 'Thousands Travel Home To Ireland To Vote On Abortion Access', 'Irish Voters Set To Liberalize Abortion Laws In Landslide, Exit Poll Signals', \"Warriors Coach Steve Kerr Calls NFL Ban On Protests 'Fake Patriotism'\", 'In Historic Victory, Barbados Elects First Female Prime Minister', 'Police Killed At Least 378 Black Americans From The Moment Colin Kaepernick Protested']\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "my_json_file=path+\"/news-category-small.json\" \n",
    "\n",
    "data = [json.loads(line) for line in open(my_json_file, 'r')] \n",
    "\n",
    "#for row in data:\n",
    "#    print(row[\"headline\"])\n",
    "\n",
    "headlines=[row[\"headline\"]  for row in data]\n",
    "\n",
    "print(len(headlines))\n",
    "print(headlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of headline is : 0\n"
     ]
    }
   ],
   "source": [
    "headlines = []\n",
    "for filename in os.listdir(path):\n",
    "    if \"Articles\" in filename:\n",
    "        article_df = pd.read_csv(path + \"/New York Times/\" + filename)\n",
    "        headlines.extend(list(article_df[\"headline\"].values))\n",
    "        break\n",
    "        \n",
    "headlines = [ h for h in headlines if h != \"Unknown\" ]\n",
    "print(\"The number of headline is :\", len(headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset preparation 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dataset cleaning  データクリーニング\n",
    "  \n",
    "記号を取り除き、「文字」と「数字」だけ残す。また小文字で統一する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def clean_text(headline):\n",
    "    text = \"\".join( word for word in headline if word not in string.punctuation ).lower()\n",
    "    text = text.encode(\"utf8\").decode(\"ascii\", \"ignore\")\n",
    "    return text\n",
    "\n",
    "# 元データのタイトルに含まれる単語群から独自のコーパスを作成\n",
    "corpus = [ clean_text(headline) for headline in headlines ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there were 2 mass shootings in texas last week but only 1 on tv',\n",
       " 'will smith joins diplo and nicky jam for the 2018 world cups official song',\n",
       " 'hugh grant marries for the first time at age 57',\n",
       " 'jim carrey blasts castrato adam schiff and democrats in new artwork',\n",
       " 'julianna margulies uses donald trump poop bags to pick up after her dog']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Generating Sequence of N-gram Tokens 文章の単語化&数値化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自然言語処理では、テキストを単語単位に分解してベクトル化するのが主流である。  \n",
    "- N-gram は、Morphological Analysis（形態要素解析）に並ぶ代表的な単語の切り出し手法のひとつ。  \n",
    "- 具体的には、N-gramとは自然言語（テキスト）を連続するN個の文字、もしくはN個の単語単位で切り出す手法のこと。  \n",
    "- 強みは「コーパス」が事前に入らないこと、弱みは切り出した単語数が肥大化しやすい点。  \n",
    "- ex:) \"I voted for Trump.\" n=2 => \"I voted\", \"for Trump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for line in corpus:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        vocab.append(word)\n",
    "\n",
    "vocabraly = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabraly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(2000)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word2index = tokenizer.word_index\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': 1,\n",
       " 'in': 2,\n",
       " 'on': 3,\n",
       " 'trump': 4,\n",
       " 'new': 5,\n",
       " 'trumps': 6,\n",
       " 'week': 7,\n",
       " 'and': 8,\n",
       " 'the': 9,\n",
       " 'at': 10,\n",
       " 'this': 11,\n",
       " 'with': 12,\n",
       " 'abortion': 13,\n",
       " '2': 14,\n",
       " 'texas': 15,\n",
       " 'but': 16,\n",
       " 'for': 17,\n",
       " 'first': 18,\n",
       " 'donald': 19,\n",
       " 'after': 20,\n",
       " 'what': 21,\n",
       " 'watch': 22,\n",
       " 'prime': 23,\n",
       " 'thats': 24,\n",
       " 'north': 25,\n",
       " 'summit': 26,\n",
       " 'of': 27,\n",
       " 'more': 28,\n",
       " 'putin': 29,\n",
       " 'than': 30,\n",
       " 'ireland': 31,\n",
       " 'landslide': 32,\n",
       " 'men': 33,\n",
       " 'he': 34,\n",
       " 'there': 35,\n",
       " 'were': 36,\n",
       " 'mass': 37,\n",
       " 'shootings': 38,\n",
       " 'last': 39,\n",
       " 'only': 40,\n",
       " '1': 41,\n",
       " 'tv': 42,\n",
       " 'will': 43,\n",
       " 'smith': 44,\n",
       " 'joins': 45,\n",
       " 'diplo': 46,\n",
       " 'nicky': 47,\n",
       " 'jam': 48,\n",
       " '2018': 49,\n",
       " 'world': 50,\n",
       " 'cups': 51,\n",
       " 'official': 52,\n",
       " 'song': 53,\n",
       " 'hugh': 54,\n",
       " 'grant': 55,\n",
       " 'marries': 56,\n",
       " 'time': 57,\n",
       " 'age': 58,\n",
       " '57': 59,\n",
       " 'jim': 60,\n",
       " 'carrey': 61,\n",
       " 'blasts': 62,\n",
       " 'castrato': 63,\n",
       " 'adam': 64,\n",
       " 'schiff': 65,\n",
       " 'democrats': 66,\n",
       " 'artwork': 67,\n",
       " 'julianna': 68,\n",
       " 'margulies': 69,\n",
       " 'uses': 70,\n",
       " 'poop': 71,\n",
       " 'bags': 72,\n",
       " 'pick': 73,\n",
       " 'up': 74,\n",
       " 'her': 75,\n",
       " 'dog': 76,\n",
       " 'morgan': 77,\n",
       " 'freeman': 78,\n",
       " 'devastated': 79,\n",
       " 'that': 80,\n",
       " 'sexual': 81,\n",
       " 'harassment': 82,\n",
       " 'claims': 83,\n",
       " 'could': 84,\n",
       " 'undermine': 85,\n",
       " 'legacy': 86,\n",
       " 'is': 87,\n",
       " 'lovin': 88,\n",
       " 'mcdonalds': 89,\n",
       " 'jingle': 90,\n",
       " 'tonight': 91,\n",
       " 'show': 92,\n",
       " 'bit': 93,\n",
       " 'amazon': 94,\n",
       " 'mike': 95,\n",
       " 'myers': 96,\n",
       " 'reveals': 97,\n",
       " 'hed': 98,\n",
       " 'like': 99,\n",
       " 'do': 100,\n",
       " 'a': 101,\n",
       " 'fourth': 102,\n",
       " 'austin': 103,\n",
       " 'powers': 104,\n",
       " 'film': 105,\n",
       " 'hulu': 106,\n",
       " 'justin': 107,\n",
       " 'timberlake': 108,\n",
       " 'visits': 109,\n",
       " 'school': 110,\n",
       " 'shooting': 111,\n",
       " 'victims': 112,\n",
       " 'south': 113,\n",
       " 'korean': 114,\n",
       " 'president': 115,\n",
       " 'meets': 116,\n",
       " 'koreas': 117,\n",
       " 'kim': 118,\n",
       " 'jong': 119,\n",
       " 'un': 120,\n",
       " 'talk': 121,\n",
       " 'its': 122,\n",
       " 'way': 123,\n",
       " 'life': 124,\n",
       " 'risk': 125,\n",
       " 'remote': 126,\n",
       " 'oystergrowing': 127,\n",
       " 'region': 128,\n",
       " 'called': 129,\n",
       " 'robots': 130,\n",
       " 'crackdown': 131,\n",
       " 'immigrant': 132,\n",
       " 'parents': 133,\n",
       " 'puts': 134,\n",
       " 'kids': 135,\n",
       " 'an': 136,\n",
       " 'already': 137,\n",
       " 'strained': 138,\n",
       " 'system': 139,\n",
       " 'son': 140,\n",
       " 'should': 141,\n",
       " 'be': 142,\n",
       " 'concerned': 143,\n",
       " 'fbi': 144,\n",
       " 'obtained': 145,\n",
       " 'wiretaps': 146,\n",
       " 'ally': 147,\n",
       " 'who': 148,\n",
       " 'met': 149,\n",
       " 'jr': 150,\n",
       " 'edward': 151,\n",
       " 'snowden': 152,\n",
       " 'theres': 153,\n",
       " 'no': 154,\n",
       " 'one': 155,\n",
       " 'loves': 156,\n",
       " 'vladimir': 157,\n",
       " 'booyah': 158,\n",
       " 'obama': 159,\n",
       " 'photographer': 160,\n",
       " 'hilariously': 161,\n",
       " 'trolls': 162,\n",
       " 'spy': 163,\n",
       " 'claim': 164,\n",
       " 'votes': 165,\n",
       " 'repeal': 166,\n",
       " 'amendment': 167,\n",
       " 'referendum': 168,\n",
       " 'ryan': 169,\n",
       " 'zinke': 170,\n",
       " 'looks': 171,\n",
       " 'reel': 172,\n",
       " 'back': 173,\n",
       " 'some': 174,\n",
       " 'critics': 175,\n",
       " 'grand': 176,\n",
       " 'pivot': 177,\n",
       " 'conservation': 178,\n",
       " 'scottish': 179,\n",
       " 'golf': 180,\n",
       " 'resort': 181,\n",
       " 'pays': 182,\n",
       " 'women': 183,\n",
       " 'significantly': 184,\n",
       " 'less': 185,\n",
       " 'report': 186,\n",
       " 'weird': 187,\n",
       " 'fathers': 188,\n",
       " 'day': 189,\n",
       " 'gifts': 190,\n",
       " 'your': 191,\n",
       " 'dad': 192,\n",
       " 'doesnt': 193,\n",
       " 'know': 194,\n",
       " 'wants': 195,\n",
       " 'does': 196,\n",
       " 'twitter': 197,\n",
       " 'putstarwarsinotherfilms': 198,\n",
       " 'it': 199,\n",
       " 'was': 200,\n",
       " 'universally': 201,\n",
       " 'entertaining': 202,\n",
       " 'mystery': 203,\n",
       " 'wolflike': 204,\n",
       " 'animal': 205,\n",
       " 'reportedly': 206,\n",
       " 'shot': 207,\n",
       " 'montana': 208,\n",
       " 'baffles': 209,\n",
       " 'wildlife': 210,\n",
       " 'officials': 211,\n",
       " 'korea': 212,\n",
       " 'still': 213,\n",
       " 'open': 214,\n",
       " 'talks': 215,\n",
       " 'cancels': 216,\n",
       " 'detonate': 217,\n",
       " 'bomb': 218,\n",
       " 'inside': 219,\n",
       " 'indian': 220,\n",
       " 'restaurant': 221,\n",
       " 'near': 222,\n",
       " 'toronto': 223,\n",
       " 'authorities': 224,\n",
       " 'say': 225,\n",
       " 'thousands': 226,\n",
       " 'travel': 227,\n",
       " 'home': 228,\n",
       " 'vote': 229,\n",
       " 'access': 230,\n",
       " 'irish': 231,\n",
       " 'voters': 232,\n",
       " 'set': 233,\n",
       " 'liberalize': 234,\n",
       " 'laws': 235,\n",
       " 'exit': 236,\n",
       " 'poll': 237,\n",
       " 'signals': 238,\n",
       " 'warriors': 239,\n",
       " 'coach': 240,\n",
       " 'steve': 241,\n",
       " 'kerr': 242,\n",
       " 'calls': 243,\n",
       " 'nfl': 244,\n",
       " 'ban': 245,\n",
       " 'protests': 246,\n",
       " 'fake': 247,\n",
       " 'patriotism': 248,\n",
       " 'historic': 249,\n",
       " 'victory': 250,\n",
       " 'barbados': 251,\n",
       " 'elects': 252,\n",
       " 'female': 253,\n",
       " 'minister': 254,\n",
       " 'police': 255,\n",
       " 'killed': 256,\n",
       " 'least': 257,\n",
       " '378': 258,\n",
       " 'black': 259,\n",
       " 'americans': 260,\n",
       " 'from': 261,\n",
       " 'moment': 262,\n",
       " 'colin': 263,\n",
       " 'kaepernick': 264,\n",
       " 'protested': 265}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "rev_dictionary = {}\n",
    "for word, idx in word2index.items():\n",
    "    if idx > 1406:\n",
    "        continue\n",
    "    dictionary[word] = idx\n",
    "    rev_dictionary[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(rev_dictionary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqences = tokenizer.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[35, 36, 14, 37, 38, 2, 15, 39, 7, 16, 40, 41, 3, 42],\n",
       " [43, 44, 45, 46, 8, 47, 48, 17, 9, 49, 50, 51, 52, 53],\n",
       " [54, 55, 56, 17, 9, 18, 57, 10, 58, 59],\n",
       " [60, 61, 62, 63, 64, 65, 8, 66, 2, 5, 67],\n",
       " [68, 69, 70, 19, 4, 71, 72, 1, 73, 74, 20, 75, 76]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_seqences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Padding the Sequences and obtain Variables\n",
    "#### パディングによって固定長データを作り、説明変数を得る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "target = []\n",
    "for line in input_seqences:\n",
    "    for i in range(1, len(line)-1):\n",
    "        input_data.append(line[:i])\n",
    "        target.append(line[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[35], [35, 36], [35, 36, 14], [35, 36, 14, 37], [35, 36, 14, 37, 38]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 37, 38, 2, 15]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 0\n",
    "for seq in input_data:\n",
    "    if len(seq) > MAX_LEN:\n",
    "        MAX_LEN = len(seq)\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "total_words= len(dictionary.keys()) \n",
    "\n",
    "input_data = pad_sequences(input_data, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 35,   0,   0, ...,   0,   0,   0],\n",
       "       [ 35,  36,   0, ...,   0,   0,   0],\n",
       "       [ 35,  36,  14, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [255, 256,  10, ...,   0,   0,   0],\n",
       "       [255, 256,  10, ...,   0,   0,   0],\n",
       "       [255, 256,  10, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 14)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-39154e121b2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'total_words' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "target = to_categorical(target, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 2001\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTMs for Text Generation 長短期記憶層アルゴリズムの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 LSTM ( Long Short-Term Memory  )    \n",
    "  \n",
    "1. Input Layer : Takes the sequence of words as input  \n",
    "2. LSTM Layer : Computes the output using LSTM units. I have added 100 units in the layer, but this number can be fine tuned later.  \n",
    "3. Dropout Layer : A regularisation layer which randomly turns-off the activations of some neurons in the LSTM layer.  \n",
    "4. Output Layer : Computes the probability of the best possible next word as output  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn-images-1.medium.com/max/1600/1*yBXV9o5q7L_CvY7quJt3WQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "\"\"\"入力層\"\"\"\n",
    "model.add(Embedding(input_dim=VOCAB_SIZE, output_dim=100, input_length=MAX_LEN))\n",
    "\n",
    "\"\"\"隠れ層\"\"\"\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "\"\"\"出力層 活性化関数は多層のソフトマックス関数\"\"\"\n",
    "model.add(Dense(units=target.shape[1], activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"損失関数と最適化手法の設定\"\"\"\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTMモデルにテストデータを学習させていく！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"予測ラベルと正解ラベルを用意する\"\"\"\n",
    "model.fit(input_data, target, batch_size=32, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 GRU ( Gated recurrent unit )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = Sequential()\n",
    "gru_model.add(Embedding(input_dim=VOCAB_SIZE, output_dim=100, input_length=MAX_LEN))\n",
    "gru_model.add(GRU(units=100))\n",
    "gru_model.add(Dropout(rate=0.1))\n",
    "gru_model.add(Dense(units=target.shape[1], activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRUモデルにテストデータを学習させていく！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.fit(input_data, target, batch_size=32, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating the text タイトルにふさわしいテキストを自動生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "set_random_seed(2)\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generater(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len, padding=\"post\")\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Trump decided\"\n",
    "text_generater(text1, 5, model, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generater(text1, 5, gru_model, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I need more data to training I guess.... 学習不足"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
